import numpy as np

# Binary ASCII (7-bit) representations for digits 0-9
ascii_digits = {
    0: [0,1,1,0,0,0,0],  # '0' = 48 in decimal
    1: [0,1,1,0,0,0,1],  # '1' = 49
    2: [0,1,1,0,0,1,0],  # '2' = 50
    3: [0,1,1,0,0,1,1],  # '3' = 51
    4: [0,1,1,0,1,0,0],  # '4' = 52
    5: [0,1,1,0,1,0,1],  # '5' = 53
    6: [0,1,1,0,1,1,0],  # '6' = 54
    7: [0,1,1,0,1,1,1],  # '7' = 55
    8: [0,1,1,1,0,0,0],  # '8' = 56
    9: [0,1,1,1,0,0,1],  # '9' = 57
}

# Even=0, Odd=1
labels = {
    0: 0,
    1: 1,
    2: 0,
    3: 1,
    4: 0,
    5: 1,
    6: 0,
    7: 1,
    8: 0,
    9: 1,
}

# Prepare dataset
X = np.array([ascii_digits[digit] for digit in range(10)])
y = np.array([labels[digit] for digit in range(10)])

# Initialize weights and bias
weights = np.zeros(7)
bias = 0
learning_rate = 0.1
epochs = 20

# Activation function: Step function
def activation(x):
    return 1 if x >= 0 else 0

# Training Perceptron
for epoch in range(epochs):
    for i in range(len(X)):
        linear_output = np.dot(X[i], weights) + bias
        y_pred = activation(linear_output)
        error = y[i] - y_pred
        
        # Update rule
        weights += learning_rate * error * X[i]
        bias += learning_rate * error

# Testing
print("Digit | Prediction (0=Even, 1=Odd)")
for digit in range(10):
    input_data = np.array(ascii_digits[digit])
    linear_output = np.dot(input_data, weights) + bias
    prediction = activation(linear_output)
    print(f"  {digit}   |      {prediction}")
